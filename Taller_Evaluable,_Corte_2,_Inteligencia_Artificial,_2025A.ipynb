{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfDqFAK1jDCoKUZpulYyZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JDNG111/iniciando_html/blob/main/Taller_Evaluable%2C_Corte_2%2C_Inteligencia_Artificial%2C_2025A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taller Evaluable\n",
        "\n",
        "Corte 2\n",
        "\n",
        "Inteligencia Artificial\n",
        "\n",
        "2025A:\n",
        "\n",
        "***Hecho por: Julian David Navarro G.***\n",
        "\n",
        "Mayo 10 del 2025"
      ],
      "metadata": {
        "id": "Kda0BFOPKoIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio 1: Configuraci√≥n del Entorno y Carga de Modelo Base***\n",
        "\n",
        "Objetivo: Establecer el entorno de desarrollo necesario para trabajar con modelos LLM y cargar un modelo pre-entrenado utilizando las bibliotecas Transformers y PyTorch."
      ],
      "metadata": {
        "id": "Vu5TpvlZKtoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQGU2I0eJ4pA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2a8fc3-1eb3-4c0a-81f0-f413dffb367b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo 'ostorc/Conversational_Spanish_GPT' cargado en cpu.\n",
            "\n",
            "--- Respuesta generada ---\n",
            "Siento escuchar eso. Te mando muchos √°nimos.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import os\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    # Carga el tokenizador y modelo\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "    # Asegura que exista pad_token\n",
        "    if tokenizador.pad_token_id is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        tokenizador.pad_token_id = tokenizador.eos_token_id\n",
        "\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "    modelo.eval()\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo.to(dispositivo)\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def main():\n",
        "    modelo_id = \"ostorc/Conversational_Spanish_GPT\"\n",
        "    modelo, tokenizador, dispositivo = cargar_modelo(modelo_id)\n",
        "    print(f\"Modelo '{modelo_id}' cargado en {dispositivo}.\")\n",
        "\n",
        "    # Entrada del usuario + token EOS\n",
        "    entrada = \"estoy triste\"\n",
        "    input_ids = tokenizador.encode(entrada + tokenizador.eos_token, return_tensors=\"pt\").to(dispositivo)\n",
        "\n",
        "    # Generaci√≥n: le decimos cu√°ntos tokens m√°s puede generar\n",
        "    with torch.no_grad():\n",
        "        chat_history = modelo.generate(\n",
        "            input_ids,\n",
        "            max_length=input_ids.shape[-1] + 50,      # hasta 50 tokens de respuesta\n",
        "            pad_token_id=tokenizador.eos_token_id\n",
        "        )\n",
        "\n",
        "    respuesta_ids = chat_history[:, input_ids.shape[-1]:][0]\n",
        "    respuesta = tokenizador.decode(respuesta_ids, skip_special_tokens=True)\n",
        "\n",
        "    print(\"\\n--- Respuesta generada ---\")\n",
        "    print(respuesta)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio 2: Procesamiento de Entrada y Generaci√≥n de Respuestas***\n",
        "\n",
        "Objetivo: Desarrollar las funciones necesarias para procesar la entrada del usuario, preparar los tokens para el modelo y generar respuestas coherentes."
      ],
      "metadata": {
        "id": "kE9DCj3_LEJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "    modelo.eval()\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo.to(dispositivo)\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def preprocesar_entrada(texto, tokenizador, dispositivo, longitud_maxima=512):\n",
        "    tokens = tokenizador.encode(\n",
        "        texto,\n",
        "        truncation=True,\n",
        "        max_length=longitud_maxima,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return tokens.to(dispositivo)\n",
        "\n",
        "def generar_respuesta(modelo, entrada_procesada, tokenizador, parametros_generacion=None):\n",
        "    if parametros_generacion is None:\n",
        "        parametros_generacion = {\n",
        "            \"max_new_tokens\": 100,\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 50,\n",
        "            \"num_return_sequences\": 1,\n",
        "            \"pad_token_id\": tokenizador.eos_token_id\n",
        "        }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        salida = modelo.generate(\n",
        "            entrada_procesada,\n",
        "            **parametros_generacion\n",
        "        )\n",
        "\n",
        "    respuesta = tokenizador.decode(salida[0], skip_special_tokens=True)\n",
        "    return respuesta\n",
        "\n",
        "def crear_prompt_sistema(instrucciones, pregunta_usuario):\n",
        "    return f\"{instrucciones}\\nUsuario: {pregunta_usuario}\\nAsistente:\"\n",
        "\n",
        "def interaccion_simple():\n",
        "    nombre_modelo = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "    modelo, tokenizador, dispositivo = cargar_modelo(nombre_modelo)\n",
        "\n",
        "    instrucciones = \"Answer as best you can\"\n",
        "    entrada_usuario = \"cuentame un chiste\"\n",
        "    prompt = crear_prompt_sistema(instrucciones, entrada_usuario)\n",
        "\n",
        "    entrada = preprocesar_entrada(prompt, tokenizador, dispositivo)\n",
        "    respuesta = generar_respuesta(modelo, entrada, tokenizador)\n",
        "\n",
        "    print(\"\\n--- Interacci√≥n con el chatbot ---\")\n",
        "    print(\"Entrada:\")\n",
        "    print(prompt)\n",
        "    print(\"\\nRespuesta generada:\")\n",
        "    print(respuesta)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interaccion_simple()\n"
      ],
      "metadata": {
        "id": "KaxxR6xtLFyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23cd77cd-5938-44dd-bbb3-d330058d4ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Interacci√≥n con el chatbot ---\n",
            "Entrada:\n",
            "Answer as best you can\n",
            "Usuario: cuentame un chiste\n",
            "Asistente:\n",
            "\n",
            "Respuesta generada:\n",
            "Answer as best you can\n",
            "Usuario: cuentame un chiste\n",
            "Asistente: ¬°Claro! Aqu√≠ tienes uno:\n",
            "\n",
            "¬øQu√© hace una paloma en un supermercado?\n",
            "\n",
            "En el supermercado, sufrir√° un cambio de vida. ¬°Con su nuevo gabinete, va a ser la palomita m√°s popular del mundo! \n",
            "\n",
            "Espero que te guste. ¬øHay algo m√°s que puedes decirme? No dudes en preguntar. ¬°Estoy aqu√≠ para ayudarte! üôå‚ú®\n",
            "\n",
            "No te pierdas m√°s chistes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio 3: Manejo de Contexto Conversacional***\n",
        "\n",
        "Objetivo: Implementar un sistema para mantener el contexto de la conversaci√≥n, permitiendo al chatbot recordar intercambios anteriores y responder coherentemente a conversaciones prolongadas."
      ],
      "metadata": {
        "id": "5PFVKbj8TI9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# === Utilidades comunes ===\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Utilizando dispositivo: {dispositivo}\")\n",
        "    return dispositivo\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    dispositivo = verificar_dispositivo()\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "\n",
        "    # Configuramos el pad_token si no est√° definido\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        tokenizador.pad_token_id = tokenizador.eos_token_id\n",
        "\n",
        "    modelo.to(dispositivo).eval()\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def preprocesar_entrada(prompt, tokenizador, dispositivo, longitud_maxima=1024):\n",
        "    return tokenizador(prompt, return_tensors=\"pt\", truncation=True, max_length=longitud_maxima).to(dispositivo)\n",
        "\n",
        "def generar_respuesta(modelo, entrada, tokenizador, prompt, parametros=None):\n",
        "    if parametros is None:\n",
        "        parametros = {\n",
        "            \"max_new_tokens\": 80,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"do_sample\": True,\n",
        "            \"pad_token_id\": tokenizador.pad_token_id,\n",
        "        }\n",
        "    with torch.no_grad():\n",
        "        salida_ids = modelo.generate(**entrada, **parametros)\n",
        "\n",
        "    texto_generado = tokenizador.decode(salida_ids[0], skip_special_tokens=True)\n",
        "    respuesta = texto_generado[len(prompt):].strip()\n",
        "    return respuesta or \"[Empty response]\"\n",
        "\n",
        "# === Gestor de contexto ===\n",
        "\n",
        "class GestorContexto:\n",
        "    \"\"\"\n",
        "    Clase para gestionar el contexto de una conversaci√≥n con el chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
        "        \"\"\"\n",
        "        Inicializa el gestor de contexto.\n",
        "\n",
        "        Args:\n",
        "            longitud_maxima (int): N√∫mero m√°ximo de tokens a mantener en el contexto\n",
        "            formato_mensaje (callable): Funci√≥n para formatear mensajes (por defecto, None)\n",
        "        \"\"\"\n",
        "        self.historial = []\n",
        "        self.longitud_maxima = longitud_maxima\n",
        "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
        "\n",
        "    def _formato_predeterminado(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Formato predeterminado para mensajes.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "\n",
        "        Returns:\n",
        "            str: Mensaje formateado\n",
        "        \"\"\"\n",
        "        if rol == \"sistema\":\n",
        "            return f\"System: {contenido}\"\n",
        "        elif rol == \"usuario\":\n",
        "            return f\"User: {contenido}\"\n",
        "        elif rol == \"asistente\":\n",
        "            return f\"Assistant: {contenido}\"\n",
        "        return contenido\n",
        "\n",
        "    def agregar_mensaje(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Agrega un mensaje al historial de conversaci√≥n.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "        \"\"\"\n",
        "        self.historial.append((rol, contenido))\n",
        "\n",
        "    def construir_prompt_completo(self):\n",
        "        \"\"\"\n",
        "        Construye un prompt completo basado en el historial.\n",
        "\n",
        "        Returns:\n",
        "            str: Prompt completo para el modelo\n",
        "        \"\"\"\n",
        "        return \"\\n\".join([self.formato_mensaje(rol, cont) for rol, cont in self.historial])\n",
        "\n",
        "    def truncar_historial(self, tokenizador):\n",
        "        \"\"\"\n",
        "        Trunca el historial si excede la longitud m√°xima.\n",
        "\n",
        "        Args:\n",
        "            tokenizador: Tokenizador del modelo\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            prompt = self.construir_prompt_completo()\n",
        "            input_ids = tokenizador(prompt, return_tensors=\"pt\", truncation=False)[\"input_ids\"]\n",
        "            if input_ids.shape[1] <= self.longitud_maxima or len(self.historial) <= 1:\n",
        "                break\n",
        "            self.historial.pop(1)  # Preserva instrucciones iniciales del sistema\n",
        "\n",
        "# === Clase Chatbot ===\n",
        "\n",
        "class Chatbot:\n",
        "    \"\"\"\n",
        "    Implementaci√≥n de chatbot con manejo de contexto.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modelo_id, instrucciones_sistema=None):\n",
        "        \"\"\"\n",
        "        Inicializa el chatbot.\n",
        "\n",
        "        Args:\n",
        "            modelo_id (str): Identificador del modelo en Hugging Face\n",
        "            instrucciones_sistema (str): Instrucciones de comportamiento del sistema\n",
        "        \"\"\"\n",
        "        self.modelo, self.tokenizador, self.dispositivo = cargar_modelo(modelo_id)\n",
        "        self.gestor_contexto = GestorContexto()\n",
        "\n",
        "        # Inicializar el contexto con instrucciones del sistema que en este caso le proporcionaremos\n",
        "        if instrucciones_sistema:\n",
        "            self.gestor_contexto.agregar_mensaje(\"sistema\", instrucciones_sistema)\n",
        "\n",
        "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
        "        \"\"\"\n",
        "        Genera una respuesta al mensaje del usuario.\n",
        "\n",
        "        Args:\n",
        "            mensaje_usuario (str): Mensaje del usuario\n",
        "            parametros_generacion (dict): Par√°metros para la generaci√≥n\n",
        "\n",
        "        Returns:\n",
        "            str: Respuesta del chatbot\n",
        "        \"\"\"\n",
        "        # 1. Agregar mensaje del usuario al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"usuario\", mensaje_usuario)\n",
        "\n",
        "        # 2. Truncar el historial si es necesario\n",
        "        self.gestor_contexto.truncar_historial(self.tokenizador)\n",
        "\n",
        "        # 3. Construir el prompt completo\n",
        "        prompt = self.gestor_contexto.construir_prompt_completo()\n",
        "\n",
        "        # 4. Preprocesar la entrada\n",
        "        entrada = preprocesar_entrada(prompt, self.tokenizador, self.dispositivo)\n",
        "\n",
        "        # 5. Generar la respuesta\n",
        "        respuesta = generar_respuesta(self.modelo, entrada, self.tokenizador, prompt, parametros_generacion)\n",
        "\n",
        "        # 6. Agregar respuesta al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"asistente\", respuesta)\n",
        "\n",
        "        # 7. Devolver la respuesta\n",
        "        return respuesta\n",
        "\n",
        "# === Prueba de conversaci√≥n ===\n",
        "\n",
        "def prueba_conversacion():\n",
        "    \"\"\"\n",
        "    Funci√≥n para probar el chatbot con una conversaci√≥n de varios turnos.\n",
        "    \"\"\"\n",
        "    # Crear una instancia del chatbot con instrucciones del sistema\n",
        "    instrucciones = \"You are a helpful AI assistant that remembers the user's name and provides long, thoughtful answers.\"\n",
        "    chatbot = Chatbot(\"Qwen/Qwen2.5-0.5B-Instruct\", instrucciones)\n",
        "\n",
        "    # Simular una conversaci√≥n de varios turnos\n",
        "    preguntas = [\n",
        "        \"Mi nombre es Julian\",\n",
        "        \"¬ørecuerdas cual es mi nombre?\", #Corroboramos de que el modelo es capaz de recordar datos en la conversaci√≥n\n",
        "        \"Cuentame un dato curioso sobre la IA\",\n",
        "        \"¬øpuedes ayudarme a estudiar para un examen?\"\n",
        "    ]\n",
        "\n",
        "    for i, pregunta in enumerate(preguntas):\n",
        "        print(f\"\\n--- Turn {i + 1} ---\")\n",
        "        print(f\"User: {pregunta}\")\n",
        "        respuesta = chatbot.responder(pregunta)\n",
        "        print(f\"Assistant: {respuesta}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prueba_conversacion()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e64MTvNUTICM",
        "outputId": "01c05a89-92a5-4312-cfb2-ff2fa620a9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizando dispositivo: cpu\n",
            "\n",
            "--- Turn 1 ---\n",
            "User: Mi nombre es Alex\n",
            "Assistant: . ¬øC√≥mo est√°s hoy? (Ingl√©s)\n",
            "Assistant: Hola, Alex! Estoy muy bien, gracias por preguntar. ¬øC√≥mo te encuentras hoy mismo? Me encanta interactuar con personas y compartir informaci√≥n sobre m√≠. ¬øEn qu√© puedo ayudarte hoy? ¬°Por supuesto! No dudes en hacerme saber c√≥mo puedo ayudarte m√°s all√° de lo que\n",
            "\n",
            "--- Turn 2 ---\n",
            "User: ¬ørecuerdas cual es mi nombre?\n",
            "Assistant: Assistant:\n",
            "\n",
            "Recuerdo que tu nombre es Alex. Como asistente digital, tengo la capacidad de recordar informaci√≥n hasta el √∫ltimo segundo. ¬øTienes alguna otra pregunta o necesitas ayuda con algo m√°s? ¬°Espero que tengas un buen d√≠a!\n",
            "\n",
            "--- Turn 3 ---\n",
            "User: Cuentame un dato curioso sobre la IA\n",
            "Assistant: .\n",
            "Assistant:\n",
            "\n",
            "¬øQui√©n fue la primera persona a tener una conversaci√≥n inteligente? La persona que llev√≥ el primer intento de conversaci√≥n inteligente en 1957. Aunque a√∫n no conocemos qui√©n era esa persona, su historia ha sido narrada y documentada en varios libros y pel√≠culas. Este evento se llam√≥ \"la conversaci√≥n inteligente\" y\n",
            "\n",
            "--- Turn 4 ---\n",
            "User: ¬øpuedes ayudarme a estudiar para un examen?\n",
            "Assistant: Assistant:\n",
            "Como asistente, estoy aqu√≠ para proporcionar informaci√≥n √∫til y responder cualquier consulta que pueda tener. Para estudiar para un examen, hay algunas estrategias que puedes seguir:\n",
            "\n",
            "1. **Revisar previamente**: Antes de tomar el examen, aseg√∫rate de revisar los temas que se presentar√°n en el examen. Puedes buscar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio 4: Optimizaci√≥n del Modelo para Recursos Limitados***\n",
        "\n",
        "Objetivo: Implementar t√©cnicas de optimizaci√≥n para mejorar la velocidad de inferencia y reducir el consumo de memoria, permitiendo que el chatbot funcione eficientemente en dispositivos con recursos limitados."
      ],
      "metadata": {
        "id": "daJ7dV7CUqaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import psutil\n",
        "import gc\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch.nn as nn\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    \"\"\"\n",
        "    Verifica y retorna el dispositivo disponible para ejecutar el modelo.\n",
        "\n",
        "    Returns:\n",
        "        torch.device: Dispositivo detectado (cuda o cpu)\n",
        "    \"\"\"\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Utilizando dispositivo: {dispositivo}\")\n",
        "    return dispositivo\n",
        "\n",
        "def configurar_cuantizacion(bits=4):\n",
        "    \"\"\"\n",
        "    Configura los par√°metros para la cuantizaci√≥n del modelo.\n",
        "\n",
        "    Args:\n",
        "        bits (int): Bits para cuantizaci√≥n (4 u 8)\n",
        "\n",
        "    Returns:\n",
        "        BitsAndBytesConfig: Configuraci√≥n de cuantizaci√≥n\n",
        "    \"\"\"\n",
        "    if bits not in [4, 8]:\n",
        "        raise ValueError(\"La cuantizaci√≥n solo soporta 4 u 8 bits\")\n",
        "\n",
        "    # Configurar la cuantizaci√≥n utilizando BitsAndBytesConfig\n",
        "    config_cuantizacion = BitsAndBytesConfig(\n",
        "        load_in_4bit=bits == 4,\n",
        "        load_in_8bit=bits == 8,\n",
        "        bnb_4bit_quant_type=\"nf4\",  # Formato de cuantizaci√≥n (normal float 4 bits)\n",
        "        bnb_4bit_compute_dtype=torch.float16,  # Tipo de datos para c√≥mputo\n",
        "        bnb_4bit_use_double_quant=True,  # Doble cuantizaci√≥n para ahorrar m√°s memoria\n",
        "    )\n",
        "\n",
        "    return config_cuantizacion\n",
        "\n",
        "def cargar_modelo_optimizado(nombre_modelo, optimizaciones=None):\n",
        "    \"\"\"\n",
        "    Carga un modelo con optimizaciones aplicadas.\n",
        "\n",
        "    Args:\n",
        "        nombre_modelo (str): Identificador del modelo\n",
        "        optimizaciones (dict): Diccionario con flags para las optimizaciones\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo, tokenizador, dispositivo)\n",
        "    \"\"\"\n",
        "    dispositivo = verificar_dispositivo()\n",
        "\n",
        "    if optimizaciones is None:\n",
        "        optimizaciones = {\n",
        "            \"cuantizacion\": torch.cuda.is_available(),  # Solo activar si hay GPU (ya que, al trabajar en Colab, no tenemos GPU)\n",
        "            \"bits\": 4,\n",
        "            \"offload_cpu\": False,\n",
        "            \"flash_attention\": torch.cuda.is_available()  # Solo activar si hay GPU\n",
        "        }\n",
        "\n",
        "    # Si estamos en CPU, desactivar optimizaciones que requieren GPU\n",
        "    if dispositivo.type == \"cpu\":\n",
        "        optimizaciones[\"cuantizacion\"] = False\n",
        "        optimizaciones[\"flash_attention\"] = False\n",
        "\n",
        "    # Preparar argumentos para cargar el modelo\n",
        "    model_args = {\n",
        "        \"pretrained_model_name_or_path\": nombre_modelo,\n",
        "        \"trust_remote_code\": True,\n",
        "    }\n",
        "\n",
        "    # Aplicar cuantizaci√≥n si est√° habilitada y hay GPU\n",
        "    if optimizaciones.get(\"cuantizacion\", False) and torch.cuda.is_available():\n",
        "        bits = optimizaciones.get(\"bits\", 4)\n",
        "        try:\n",
        "            model_args[\"quantization_config\"] = configurar_cuantizacion(bits)\n",
        "        except ImportError:\n",
        "            print(\"La librer√≠a bitsandbytes no est√° disponible. Desactivando cuantizaci√≥n.\")\n",
        "            optimizaciones[\"cuantizacion\"] = False\n",
        "\n",
        "    # Configurar offloading a CPU si est√° habilitado\n",
        "    if optimizaciones.get(\"offload_cpu\", False):\n",
        "        if torch.cuda.is_available():\n",
        "            model_args[\"device_map\"] = \"auto\"\n",
        "            model_args[\"offload_folder\"] = \"offload_folder\"\n",
        "            os.makedirs(\"offload_folder\", exist_ok=True)\n",
        "\n",
        "    # Configurar atenci√≥n flash si est√° habilitada y disponible en hardware\n",
        "    if optimizaciones.get(\"flash_attention\", False) and torch.cuda.is_available():\n",
        "        try:\n",
        "            model_args[\"use_flash_attention_2\"] = True\n",
        "        except Exception as e:\n",
        "            print(f\"Flash Attention no pudo ser habilitada: {e}\")\n",
        "            optimizaciones[\"flash_attention\"] = False\n",
        "\n",
        "    # Cargar el tokenizador\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo, trust_remote_code=True)\n",
        "\n",
        "    # Configurar pad_token si no est√° definido\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        tokenizador.pad_token_id = tokenizador.eos_token_id\n",
        "\n",
        "    # Para CPUs, usar optimizaciones de CPU\n",
        "    if dispositivo.type == \"cpu\":\n",
        "        try:\n",
        "            model_args[\"torch_dtype\"] = torch.float16\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Cargar el modelo con las optimizaciones\n",
        "    try:\n",
        "        print(f\"Cargando modelo con optimizaciones: {optimizaciones}\")\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(**model_args)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo con optimizaciones. Intentando cargar sin optimizaciones: {e}\")\n",
        "        # Intento de respaldo: cargar modelo sin optimizaciones\n",
        "        backup_args = {\n",
        "            \"pretrained_model_name_or_path\": nombre_modelo,\n",
        "            \"trust_remote_code\": True\n",
        "        }\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(**backup_args)\n",
        "\n",
        "    # Mover el modelo al dispositivo (si no se usa device_map='auto')\n",
        "    if not optimizaciones.get(\"offload_cpu\", False):\n",
        "        modelo = modelo.to(dispositivo)\n",
        "\n",
        "    # Establecer el modo de evaluaci√≥n\n",
        "    modelo.eval()\n",
        "\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def aplicar_sliding_window(modelo, window_size=1024):\n",
        "    \"\"\"\n",
        "    Configura la atenci√≥n de ventana deslizante para procesar secuencias largas.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a configurar\n",
        "        window_size (int): Tama√±o de la ventana de atenci√≥n\n",
        "    \"\"\"\n",
        "    # Verificar que el modelo tenga configuraci√≥n de atenci√≥n\n",
        "    try:\n",
        "        if hasattr(modelo.config, \"sliding_window\"):\n",
        "            # Configurar sliding window attention\n",
        "            modelo.config.sliding_window = window_size\n",
        "            modelo.config.use_sliding_window = True\n",
        "            print(f\"Sliding window configurada con tama√±o {window_size}\")\n",
        "        else:\n",
        "            print(\"Este modelo no soporta sliding window attention de forma directa\")\n",
        "\n",
        "            # Intento alternativo para modelos que no tienen el atributo directamente\n",
        "            try:\n",
        "                for layer in modelo.model.layers:\n",
        "                    if hasattr(layer.self_attn, \"sliding_window\"):\n",
        "                        layer.self_attn.sliding_window = window_size\n",
        "                print(f\"Sliding window configurada manualmente con tama√±o {window_size}\")\n",
        "            except:\n",
        "                print(\"No se pudo configurar sliding window para este modelo\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al configurar sliding window: {e}\")\n",
        "\n",
        "def aplicar_optimizaciones_cpu(modelo):\n",
        "    \"\"\"\n",
        "    Aplica optimizaciones espec√≠ficas para CPU.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a optimizar\n",
        "    \"\"\"\n",
        "    # Fusi√≥n de operaciones cuando sea posible\n",
        "    try:\n",
        "        for module in modelo.modules():\n",
        "            if isinstance(module, nn.Sequential):\n",
        "                # Intentar fusionar operaciones secuenciales\n",
        "                torch.jit.script(module)\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo aplicar fusi√≥n de operaciones: {e}\")\n",
        "\n",
        "    try:\n",
        "        if hasattr(modelo, \"half\"):\n",
        "            modelo = modelo.half()\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo convertir a half precision: {e}\")\n",
        "\n",
        "    return modelo\n",
        "\n",
        "def medir_uso_memoria():\n",
        "    \"\"\"\n",
        "    Mide el uso actual de memoria.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (uso de RAM en MB, uso de VRAM en MB si disponible)\n",
        "    \"\"\"\n",
        "    # Medimos el uso de RAM\n",
        "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)  # MB\n",
        "\n",
        "    # Medimos el uso de VRAM si hay GPU disponible\n",
        "    vram_usage = 0\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        vram_usage = torch.cuda.memory_allocated() / (1024 * 1024)  # MB\n",
        "\n",
        "    return ram_usage, vram_usage\n",
        "\n",
        "def evaluar_rendimiento(modelo, tokenizador, texto_prueba, dispositivo):\n",
        "    \"\"\"\n",
        "    Eval√∫a el rendimiento del modelo en t√©rminos de velocidad y memoria.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a evaluar\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        texto_prueba (str): Texto para pruebas de rendimiento\n",
        "        dispositivo: Dispositivo donde se ejecutar√°\n",
        "\n",
        "    Returns:\n",
        "        dict: M√©tricas de rendimiento\n",
        "    \"\"\"\n",
        "    # Asegurar que no hay c√°lculos pendientes en la GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Limpiar cach√© para medici√≥n m√°s precisa\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Guardar uso de memoria antes de la inferencia\n",
        "    ram_antes, vram_antes = medir_uso_memoria()\n",
        "\n",
        "    # Preparar la entrada\n",
        "    inputs = tokenizador(texto_prueba, return_tensors=\"pt\").to(dispositivo)\n",
        "    input_tokens = inputs.input_ids.shape[1]\n",
        "\n",
        "    # Calentar el modelo con una inferencia inicial\n",
        "    with torch.no_grad():\n",
        "        _ = modelo.generate(**inputs, max_new_tokens=10)\n",
        "\n",
        "    # Medir tiempo de inferencia (promedio de 3 ejecuciones)\n",
        "    tiempo_total = 0\n",
        "    num_ejecuciones = 3\n",
        "    output_tokens = 0\n",
        "\n",
        "    for _ in range(num_ejecuciones):\n",
        "        inicio = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = modelo.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.95\n",
        "            )\n",
        "        fin = time.time()\n",
        "\n",
        "        # Acumular tiempo y tokens generados\n",
        "        tiempo_total += (fin - inicio)\n",
        "        output_tokens = outputs.shape[1] - input_tokens  # Tokens generados (excluye entrada)\n",
        "\n",
        "    # Calcular promedios\n",
        "    tiempo_inferencia = tiempo_total / num_ejecuciones\n",
        "    tokens_por_segundo = output_tokens / tiempo_inferencia\n",
        "\n",
        "    # Medir uso de memoria despu√©s de la inferencia\n",
        "    ram_despues, vram_despues = medir_uso_memoria()\n",
        "\n",
        "    # Calcular m√©tricas\n",
        "    metricas = {\n",
        "        \"tiempo_inferencia_segundos\": tiempo_inferencia,\n",
        "        \"tokens_generados\": output_tokens,\n",
        "        \"tokens_por_segundo\": tokens_por_segundo,\n",
        "        \"uso_ram_mb\": ram_despues - ram_antes,\n",
        "        \"uso_vram_mb\": vram_despues - vram_antes if torch.cuda.is_available() else 0,\n",
        "        \"dispositivo\": str(dispositivo)\n",
        "    }\n",
        "\n",
        "    return metricas\n",
        "\n",
        "def mostrar_metricas(nombre, metricas):\n",
        "    \"\"\"\n",
        "    Muestra las m√©tricas de rendimiento de forma legible.\n",
        "\n",
        "    Args:\n",
        "        nombre (str): Nombre de la configuraci√≥n\n",
        "        metricas (dict): M√©tricas de rendimiento\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- M√©tricas para {nombre} ---\")\n",
        "    print(f\"‚Ä¢ Dispositivo: {metricas['dispositivo']}\")\n",
        "    print(f\"‚Ä¢ Tiempo de inferencia: {metricas['tiempo_inferencia_segundos']:.4f} segundos\")\n",
        "    print(f\"‚Ä¢ Tokens generados: {metricas['tokens_generados']}\")\n",
        "    print(f\"‚Ä¢ Velocidad: {metricas['tokens_por_segundo']:.2f} tokens/segundo\")\n",
        "    print(f\"‚Ä¢ Uso de RAM: {metricas['uso_ram_mb']:.2f} MB\")\n",
        "\n",
        "    if metricas['uso_vram_mb'] > 0:\n",
        "        print(f\"‚Ä¢ Uso de VRAM: {metricas['uso_vram_mb']:.2f} MB\")\n",
        "\n",
        "def demo_optimizaciones():\n",
        "    \"\"\"\n",
        "    Demuestra y compara diferentes configuraciones de optimizaci√≥n.\n",
        "    \"\"\"\n",
        "    # Texto de prueba (suficientemente largo para evaluar rendimiento)\n",
        "    texto_prueba = \"\"\"\n",
        "    La inteligencia artificial (IA) es un campo de la inform√°tica que se centra en la creaci√≥n de\n",
        "    m√°quinas capaces de imitar comportamientos inteligentes. Incluye subcampos como el aprendizaje\n",
        "    autom√°tico, el procesamiento del lenguaje natural, la visi√≥n por computadora y la rob√≥tica.\n",
        "    En los √∫ltimos a√±os, hemos visto avances significativos en modelos de lenguaje que pueden\n",
        "    comprender y generar texto similar al humano. Estos modelos tienen aplicaciones en asistentes\n",
        "    virtuales, traducci√≥n, resumen de textos y muchas otras √°reas.\n",
        "    \"\"\"\n",
        "\n",
        "    # Configuraciones a probar (usamos un modelo peque√±o para facilitar las pruebas)\n",
        "    modelo_base = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Elegimos un modelo peque√±o de la familia Qwen\n",
        "    dispositivo = verificar_dispositivo()\n",
        "\n",
        "    # Adaptar las configuraciones seg√∫n el dispositivo disponible\n",
        "    configs_a_probar = []\n",
        "\n",
        "    # 1. Modelo base sin optimizaciones (funciona en CPU y GPU)\n",
        "    configs_a_probar.append({\n",
        "        \"nombre\": \"Modelo Base\",\n",
        "        \"optimizaciones\": {\n",
        "            \"cuantizacion\": False,\n",
        "            \"flash_attention\": False,\n",
        "            \"offload_cpu\": False\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # 2. Para GPU: A√±adir configuraciones que requieren GPU\n",
        "    if dispositivo.type == \"cuda\":\n",
        "        # Modelo con cuantizaci√≥n de 4 bits\n",
        "        configs_a_probar.append({\n",
        "            \"nombre\": \"Modelo con cuantizaci√≥n 4-bit\",\n",
        "            \"optimizaciones\": {\n",
        "                \"cuantizacion\": True,\n",
        "                \"bits\": 4,\n",
        "                \"flash_attention\": False,\n",
        "                \"offload_cpu\": False\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # Modelo con Flash Attention\n",
        "        configs_a_probar.append({\n",
        "            \"nombre\": \"Modelo con Flash Attention\",\n",
        "            \"optimizaciones\": {\n",
        "                \"cuantizacion\": False,\n",
        "                \"flash_attention\": True,\n",
        "                \"offload_cpu\": False\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # Modelo con todas las optimizaciones\n",
        "        configs_a_probar.append({\n",
        "            \"nombre\": \"Modelo con todas las optimizaciones\",\n",
        "            \"optimizaciones\": {\n",
        "                \"cuantizacion\": True,\n",
        "                \"bits\": 4,\n",
        "                \"flash_attention\": True,\n",
        "                \"offload_cpu\": True\n",
        "            }\n",
        "        })\n",
        "    # 3. Para CPU: A√±adir configuraciones espec√≠ficas para CPU\n",
        "    else:\n",
        "        # Modelo con optimizaciones espec√≠ficas para CPU\n",
        "        configs_a_probar.append({\n",
        "            \"nombre\": \"Modelo optimizado para CPU (half precision)\",\n",
        "            \"optimizaciones\": {\n",
        "                \"cuantizacion\": False,\n",
        "                \"flash_attention\": False,\n",
        "                \"offload_cpu\": False,\n",
        "                \"half_precision\": True\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # 4. Modelo con sliding window (funciona en CPU y GPU)\n",
        "    configs_a_probar.append({\n",
        "        \"nombre\": \"Modelo con Sliding Window\",\n",
        "        \"optimizaciones\": {\n",
        "            \"cuantizacion\": False,\n",
        "            \"flash_attention\": False,\n",
        "            \"offload_cpu\": False,\n",
        "            \"sliding_window\": True\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # Ejecutar pruebas para cada configuraci√≥n\n",
        "    resultados = []\n",
        "\n",
        "    for config in configs_a_probar:\n",
        "        print(f\"\\n=== {config['nombre']} ===\")\n",
        "        try:\n",
        "            modelo, tokenizador, dispositivo = cargar_modelo_optimizado(\n",
        "                modelo_base,\n",
        "                optimizaciones=config[\"optimizaciones\"]\n",
        "            )\n",
        "\n",
        "            # Aplicar sliding window si est√° especificado\n",
        "            if config[\"optimizaciones\"].get(\"sliding_window\", False):\n",
        "                aplicar_sliding_window(modelo, window_size=256)\n",
        "\n",
        "            # Aplicar optimizaciones espec√≠ficas para CPU si estamos en CPU\n",
        "            if dispositivo.type == \"cpu\" and config[\"optimizaciones\"].get(\"half_precision\", False):\n",
        "                modelo = aplicar_optimizaciones_cpu(modelo)\n",
        "\n",
        "            # Evaluar rendimiento\n",
        "            metricas = evaluar_rendimiento(modelo, tokenizador, texto_prueba, dispositivo)\n",
        "            mostrar_metricas(config[\"nombre\"], metricas)\n",
        "            resultados.append((config[\"nombre\"], metricas))\n",
        "\n",
        "            # Limpiar memoria\n",
        "            del modelo\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al probar {config['nombre']}: {e}\")\n",
        "\n",
        "    # Comparar resultados si hay m√°s de una configuraci√≥n exitosa\n",
        "    if len(resultados) > 1:\n",
        "        print(\"\\n=== Comparaci√≥n de Rendimiento ===\")\n",
        "\n",
        "        # Usar el primer resultado como base para la comparaci√≥n\n",
        "        base_nombre, base_metricas = resultados[0]\n",
        "        base_tiempo = base_metricas[\"tiempo_inferencia_segundos\"]\n",
        "        base_tokens = base_metricas[\"tokens_por_segundo\"]\n",
        "        base_ram = base_metricas[\"uso_ram_mb\"]\n",
        "        base_vram = base_metricas[\"uso_vram_mb\"]\n",
        "\n",
        "        print(\"\\nMejora relativa (comparado con modelo base):\")\n",
        "        for nombre, met in resultados:\n",
        "            if nombre == base_nombre:\n",
        "                continue\n",
        "\n",
        "            speedup = base_tiempo / met[\"tiempo_inferencia_segundos\"] if met[\"tiempo_inferencia_segundos\"] > 0 else 0\n",
        "            tokens_mejora = met[\"tokens_por_segundo\"] / base_tokens if base_tokens > 0 else 0\n",
        "            ram_reduccion = base_ram / met[\"uso_ram_mb\"] if met[\"uso_ram_mb\"] > 0 else 1\n",
        "\n",
        "            print(f\"\\n‚Ä¢ {nombre}:\")\n",
        "            print(f\"  - Velocidad: {speedup:.2f}x m√°s r√°pido\")\n",
        "            print(f\"  - Throughput: {tokens_mejora:.2f}x m√°s tokens/segundo\")\n",
        "            print(f\"  - Uso de RAM: {ram_reduccion:.2f}x m√°s eficiente\")\n",
        "\n",
        "            if torch.cuda.is_available() and base_vram > 0 and met[\"uso_vram_mb\"] > 0:\n",
        "                vram_reduccion = base_vram / met[\"uso_vram_mb\"] if met[\"uso_vram_mb\"] > 0 else 1\n",
        "                print(f\"  - Uso de VRAM: {vram_reduccion:.2f}x m√°s eficiente\")\n",
        "    else:\n",
        "        print(\"\\nNo hay suficientes configuraciones exitosas para comparar.\")\n",
        "\n",
        "# Clase GestorContexto b√°sica para integraci√≥n con el chatbot\n",
        "class GestorContexto:\n",
        "    \"\"\"\n",
        "    Clase para gestionar el contexto de una conversaci√≥n con el chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
        "        \"\"\"\n",
        "        Inicializa el gestor de contexto.\n",
        "\n",
        "        Args:\n",
        "            longitud_maxima (int): N√∫mero m√°ximo de tokens a mantener en el contexto\n",
        "            formato_mensaje (callable): Funci√≥n para formatear mensajes (por defecto, None)\n",
        "        \"\"\"\n",
        "        self.historial = []\n",
        "        self.longitud_maxima = longitud_maxima\n",
        "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
        "\n",
        "    def _formato_predeterminado(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Formato predeterminado para mensajes, optimizado para modelos Qwen.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "\n",
        "        Returns:\n",
        "            str: Mensaje formateado\n",
        "        \"\"\"\n",
        "        if rol == \"sistema\":\n",
        "            return f\"<|im_start|>system\\n{contenido}<|im_end|>\"\n",
        "        elif rol == \"usuario\":\n",
        "            return f\"<|im_start|>user\\n{contenido}<|im_end|>\"\n",
        "        elif rol == \"asistente\":\n",
        "            return f\"<|im_start|>assistant\\n{contenido}<|im_end|>\"\n",
        "        return contenido\n",
        "\n",
        "    def agregar_mensaje(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Agrega un mensaje al historial de conversaci√≥n.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "        \"\"\"\n",
        "        self.historial.append((rol, contenido))\n",
        "\n",
        "    def construir_prompt_completo(self):\n",
        "        \"\"\"\n",
        "        Construye un prompt completo basado en el historial.\n",
        "\n",
        "        Returns:\n",
        "            str: Prompt completo para el modelo\n",
        "        \"\"\"\n",
        "        return \"\\n\".join([self.formato_mensaje(rol, cont) for rol, cont in self.historial])\n",
        "\n",
        "    def truncar_historial(self, tokenizador):\n",
        "        \"\"\"\n",
        "        Trunca el historial si excede la longitud m√°xima.\n",
        "\n",
        "        Args:\n",
        "            tokenizador: Tokenizador del modelo\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            prompt = self.construir_prompt_completo()\n",
        "            input_ids = tokenizador(prompt, return_tensors=\"pt\", truncation=False)[\"input_ids\"]\n",
        "            if input_ids.shape[1] <= self.longitud_maxima or len(self.historial) <= 1:\n",
        "                break\n",
        "\n",
        "            self.historial.pop(1)  # Guarda las instrucciones iniciales del sistema\n",
        "\n",
        "# Clase Chatbot adaptada para usar modelo optimizado\n",
        "class Chatbot:\n",
        "    \"\"\"\n",
        "    Implementaci√≥n de chatbot con manejo de contexto y optimizaciones.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modelo, tokenizador, dispositivo, instrucciones_sistema=None):\n",
        "        \"\"\"\n",
        "        Inicializa el chatbot con componentes pre-cargados.\n",
        "\n",
        "        Args:\n",
        "            modelo: Modelo de lenguaje pre-cargado\n",
        "            tokenizador: Tokenizador pre-cargado\n",
        "            dispositivo: Dispositivo (CPU/GPU)\n",
        "            instrucciones_sistema (str): Instrucciones iniciales\n",
        "        \"\"\"\n",
        "        self.modelo = modelo\n",
        "        self.tokenizador = tokenizador\n",
        "        self.dispositivo = dispositivo\n",
        "\n",
        "        # Crear gestor de contexto\n",
        "        self.gestor_contexto = GestorContexto()\n",
        "\n",
        "        # Inicializar con instrucciones del sistema\n",
        "        if instrucciones_sistema:\n",
        "            self.gestor_contexto.agregar_mensaje(\"sistema\", instrucciones_sistema)\n",
        "\n",
        "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
        "        \"\"\"\n",
        "        Genera una respuesta optimizada al mensaje del usuario.\n",
        "\n",
        "        Args:\n",
        "            mensaje_usuario (str): Mensaje del usuario\n",
        "            parametros_generacion (dict): Par√°metros para la generaci√≥n\n",
        "\n",
        "        Returns:\n",
        "            str: Respuesta del chatbot\n",
        "        \"\"\"\n",
        "        # Preparar par√°metros de generaci√≥n\n",
        "        if parametros_generacion is None:\n",
        "            parametros_generacion = {\n",
        "                \"max_new_tokens\": 150,\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_p\": 0.9,\n",
        "                \"do_sample\": True,\n",
        "                \"repetition_penalty\": 1.1,\n",
        "                \"pad_token_id\": self.tokenizador.pad_token_id,\n",
        "            }\n",
        "\n",
        "        # 1. Agregar mensaje del usuario al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"usuario\", mensaje_usuario)\n",
        "\n",
        "        # 2. Truncar el historial si es necesario\n",
        "        self.gestor_contexto.truncar_historial(self.tokenizador)\n",
        "\n",
        "        # 3. Construir el prompt completo\n",
        "        prompt = self.gestor_contexto.construir_prompt_completo()\n",
        "\n",
        "        # 4. Preprocesar la entrada\n",
        "        if not \"<|im_start|>assistant\" in prompt:\n",
        "            prompt += \"\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "        entrada = self.tokenizador(prompt, return_tensors=\"pt\", truncation=True).to(self.dispositivo)\n",
        "\n",
        "        # 5. Generar la respuesta\n",
        "        with torch.no_grad():\n",
        "            salida_ids = self.modelo.generate(\n",
        "                **entrada,\n",
        "                **parametros_generacion\n",
        "            )\n",
        "\n",
        "        # 6. Decodificar la respuesta\n",
        "        texto_generado = self.tokenizador.decode(salida_ids[0], skip_special_tokens=True)\n",
        "        respuesta = texto_generado[len(prompt):].strip()\n",
        "\n",
        "        # 7. Limpiar la respuesta\n",
        "        respuesta = respuesta.replace(\"#AIHelp\", \"\").strip()\n",
        "        if respuesta.startswith(\"Assistant:\"):\n",
        "            respuesta = respuesta[len(\"Assistant:\"):].strip()\n",
        "\n",
        "        # 8. Agregar respuesta al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"asistente\", respuesta)\n",
        "\n",
        "        return respuesta or \"[Empty response]\"\n",
        "\n",
        "# Creamos el chatbot optimizado con el modelo espec√≠fico\n",
        "def crear_chatbot_optimizado(modelo_id, instrucciones_sistema=None, optimizaciones=None):\n",
        "    \"\"\"\n",
        "    Crea una instancia de chatbot con optimizaciones aplicadas.\n",
        "\n",
        "    Args:\n",
        "        modelo_id (str): ID del modelo en Hugging Face\n",
        "        instrucciones_sistema (str): Instrucciones iniciales del sistema\n",
        "        optimizaciones (dict): Configuraci√≥n de optimizaciones\n",
        "\n",
        "    Returns:\n",
        "        Chatbot: Instancia del chatbot optimizado\n",
        "    \"\"\"\n",
        "    # Cargar modelo optimizado\n",
        "    modelo, tokenizador, dispositivo = cargar_modelo_optimizado(modelo_id, optimizaciones)\n",
        "\n",
        "    # Aplicar sliding window si est√° especificado\n",
        "    if optimizaciones and optimizaciones.get(\"sliding_window\", False):\n",
        "        aplicar_sliding_window(modelo, window_size=256)\n",
        "\n",
        "    # Aplicar optimizaciones espec√≠ficas para CPU si estamos en CPU\n",
        "    if dispositivo.type == \"cpu\" and optimizaciones and optimizaciones.get(\"half_precision\", False):\n",
        "        modelo = aplicar_optimizaciones_cpu(modelo)\n",
        "\n",
        "    # Crear chatbot\n",
        "    chatbot = Chatbot(modelo, tokenizador, dispositivo, instrucciones_sistema)\n",
        "\n",
        "    return chatbot\n",
        "\n",
        "# Ejecutamos\n",
        "if __name__ == \"__main__\":\n",
        "    demo_optimizaciones()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktgM79XbVxh8",
        "outputId": "b0694a96-aac8-4738-aaeb-50916449fa86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizando dispositivo: cpu\n",
            "\n",
            "=== Modelo Base ===\n",
            "Utilizando dispositivo: cpu\n",
            "Cargando modelo con optimizaciones: {'cuantizacion': False, 'flash_attention': False, 'offload_cpu': False}\n",
            "\n",
            "--- M√©tricas para Modelo Base ---\n",
            "‚Ä¢ Dispositivo: cpu\n",
            "‚Ä¢ Tiempo de inferencia: 21.1676 segundos\n",
            "‚Ä¢ Tokens generados: 50\n",
            "‚Ä¢ Velocidad: 2.36 tokens/segundo\n",
            "‚Ä¢ Uso de RAM: 0.00 MB\n",
            "\n",
            "=== Modelo optimizado para CPU (half precision) ===\n",
            "Utilizando dispositivo: cpu\n",
            "Cargando modelo con optimizaciones: {'cuantizacion': False, 'flash_attention': False, 'offload_cpu': False, 'half_precision': True}\n",
            "\n",
            "--- M√©tricas para Modelo optimizado para CPU (half precision) ---\n",
            "‚Ä¢ Dispositivo: cpu\n",
            "‚Ä¢ Tiempo de inferencia: 21.7830 segundos\n",
            "‚Ä¢ Tokens generados: 50\n",
            "‚Ä¢ Velocidad: 2.30 tokens/segundo\n",
            "‚Ä¢ Uso de RAM: 0.00 MB\n",
            "\n",
            "=== Modelo con Sliding Window ===\n",
            "Utilizando dispositivo: cpu\n",
            "Cargando modelo con optimizaciones: {'cuantizacion': False, 'flash_attention': False, 'offload_cpu': False, 'sliding_window': True}\n",
            "Sliding window configurada con tama√±o 256\n",
            "\n",
            "--- M√©tricas para Modelo con Sliding Window ---\n",
            "‚Ä¢ Dispositivo: cpu\n",
            "‚Ä¢ Tiempo de inferencia: 21.7837 segundos\n",
            "‚Ä¢ Tokens generados: 50\n",
            "‚Ä¢ Velocidad: 2.30 tokens/segundo\n",
            "‚Ä¢ Uso de RAM: 0.00 MB\n",
            "\n",
            "=== Comparaci√≥n de Rendimiento ===\n",
            "\n",
            "Mejora relativa (comparado con modelo base):\n",
            "\n",
            "‚Ä¢ Modelo optimizado para CPU (half precision):\n",
            "  - Velocidad: 0.97x m√°s r√°pido\n",
            "  - Throughput: 0.97x m√°s tokens/segundo\n",
            "  - Uso de RAM: 1.00x m√°s eficiente\n",
            "\n",
            "‚Ä¢ Modelo con Sliding Window:\n",
            "  - Velocidad: 0.97x m√°s r√°pido\n",
            "  - Throughput: 0.97x m√°s tokens/segundo\n",
            "  - Uso de RAM: 1.00x m√°s eficiente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio 5: Personalizaci√≥n del Chatbot y Despliegue***\n",
        "\n",
        "Objetivo: Implementar t√©cnicas para personalizar el comportamiento del chatbot y prepararlo para su despliegue como una aplicaci√≥n web simple."
      ],
      "metadata": {
        "id": "T6AoBIyLYSbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Necesario para ejecutar la web del ejercicio 5\n",
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77idSFGEYnzk",
        "outputId": "ef9319b4-db05-4026-8329-5bdc839dfeca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# === Utilidades comunes ===\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Utilizando dispositivo: {dispositivo}\")\n",
        "    return dispositivo\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    dispositivo = verificar_dispositivo()\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "\n",
        "    # Configurar pad_token si no est√° definido\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        tokenizador.pad_token_id = tokenizador.eos_token_id\n",
        "\n",
        "    modelo.to(dispositivo).eval()\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def preprocesar_entrada(prompt, tokenizador, dispositivo, longitud_maxima=1024):\n",
        "    return tokenizador(prompt, return_tensors=\"pt\", truncation=True, max_length=longitud_maxima).to(dispositivo)\n",
        "\n",
        "def generar_respuesta(modelo, entrada, tokenizador, prompt, parametros=None):\n",
        "    if parametros is None:\n",
        "        parametros = {\n",
        "            \"max_new_tokens\": 80,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"do_sample\": True,\n",
        "            \"pad_token_id\": tokenizador.pad_token_id,\n",
        "        }\n",
        "    with torch.no_grad():\n",
        "        salida_ids = modelo.generate(**entrada, **parametros)\n",
        "\n",
        "    texto_generado = tokenizador.decode(salida_ids[0], skip_special_tokens=True)\n",
        "    respuesta = texto_generado[len(prompt):].strip()\n",
        "    return respuesta or \"[Empty response]\"\n",
        "\n",
        "# === Gestor de contexto ===\n",
        "\n",
        "class GestorContexto:\n",
        "    \"\"\"\n",
        "    Clase para gestionar el contexto de una conversaci√≥n con el chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
        "        \"\"\"\n",
        "        Inicializa el gestor de contexto.\n",
        "\n",
        "        Args:\n",
        "            longitud_maxima (int): N√∫mero m√°ximo de tokens a mantener en el contexto\n",
        "            formato_mensaje (callable): Funci√≥n para formatear mensajes (por defecto, None)\n",
        "        \"\"\"\n",
        "        self.historial = []\n",
        "        self.longitud_maxima = longitud_maxima\n",
        "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
        "\n",
        "    def _formato_predeterminado(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Formato predeterminado para mensajes.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "\n",
        "        Returns:\n",
        "            str: Mensaje formateado\n",
        "        \"\"\"\n",
        "        if rol == \"sistema\":\n",
        "            return f\"System: {contenido}\"\n",
        "        elif rol == \"usuario\":\n",
        "            return f\"User: {contenido}\"\n",
        "        elif rol == \"asistente\":\n",
        "            return f\"Assistant: {contenido}\"\n",
        "        return contenido\n",
        "\n",
        "    def agregar_mensaje(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Agrega un mensaje al historial de conversaci√≥n.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "        \"\"\"\n",
        "        self.historial.append((rol, contenido))\n",
        "\n",
        "    def construir_prompt_completo(self):\n",
        "        \"\"\"\n",
        "        Construye un prompt completo basado en el historial.\n",
        "\n",
        "        Returns:\n",
        "            str: Prompt completo para el modelo\n",
        "        \"\"\"\n",
        "        return \"\\n\".join([self.formato_mensaje(rol, cont) for rol, cont in self.historial])\n",
        "\n",
        "    def truncar_historial(self, tokenizador):\n",
        "        \"\"\"\n",
        "        Trunca el historial si excede la longitud m√°xima.\n",
        "\n",
        "        Args:\n",
        "            tokenizador: Tokenizador del modelo\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            prompt = self.construir_prompt_completo()\n",
        "            input_ids = tokenizador(prompt, return_tensors=\"pt\", truncation=False)[\"input_ids\"]\n",
        "            if input_ids.shape[1] <= self.longitud_maxima or len(self.historial) <= 1:\n",
        "                break\n",
        "            self.historial.pop(1)  # Preserva instrucciones iniciales del sistema\n",
        "\n",
        "# === Clase Chatbot ===\n",
        "\n",
        "class Chatbot:\n",
        "    \"\"\"\n",
        "    Implementaci√≥n de chatbot con manejo de contexto.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modelo, tokenizador, dispositivo, instrucciones_sistema=None):\n",
        "        \"\"\"\n",
        "        Inicializa el chatbot.\n",
        "\n",
        "        Args:\n",
        "            modelo: Modelo cargado\n",
        "            tokenizador: Tokenizador del modelo\n",
        "            dispositivo: Dispositivo de procesamiento (CPU/GPU)\n",
        "            instrucciones_sistema (str): Instrucciones de comportamiento del sistema\n",
        "        \"\"\"\n",
        "        self.modelo = modelo\n",
        "        self.tokenizador = tokenizador\n",
        "        self.dispositivo = dispositivo\n",
        "        self.gestor_contexto = GestorContexto()\n",
        "\n",
        "        # Inicializar el contexto con instrucciones del sistema\n",
        "        if instrucciones_sistema:\n",
        "            self.gestor_contexto.agregar_mensaje(\"sistema\", instrucciones_sistema)\n",
        "\n",
        "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
        "        \"\"\"\n",
        "        Genera una respuesta al mensaje del usuario.\n",
        "\n",
        "        Args:\n",
        "            mensaje_usuario (str): Mensaje del usuario\n",
        "            parametros_generacion (dict): Par√°metros para la generaci√≥n\n",
        "\n",
        "        Returns:\n",
        "            str: Respuesta del chatbot\n",
        "        \"\"\"\n",
        "        # 1. Agregar mensaje del usuario al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"usuario\", mensaje_usuario)\n",
        "\n",
        "        # 2. Truncar el historial si es necesario\n",
        "        self.gestor_contexto.truncar_historial(self.tokenizador)\n",
        "\n",
        "        # 3. Construir el prompt completo\n",
        "        prompt = self.gestor_contexto.construir_prompt_completo()\n",
        "\n",
        "        # 4. Preprocesar la entrada\n",
        "        entrada = preprocesar_entrada(prompt, self.tokenizador, self.dispositivo)\n",
        "\n",
        "        # 5. Generar la respuesta\n",
        "        respuesta = generar_respuesta(self.modelo, entrada, self.tokenizador, prompt, parametros_generacion)\n",
        "\n",
        "        # 6. Agregar respuesta al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"asistente\", respuesta)\n",
        "\n",
        "        # 7. Devolver la respuesta\n",
        "        return respuesta\n",
        "\n",
        "# === Personalizaci√≥n con PEFT/LoRA ===\n",
        "\n",
        "def configurar_peft(modelo, r=8, lora_alpha=32):\n",
        "    \"\"\"\n",
        "    Configura el modelo para fine-tuning con PEFT/LoRA.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo base\n",
        "        r (int): Rango de adaptadores LoRA\n",
        "        lora_alpha (int): Escala alpha para LoRA\n",
        "\n",
        "    Returns:\n",
        "        modelo: Modelo adaptado para fine-tuning\n",
        "    \"\"\"\n",
        "    # Identificar autom√°ticamente los m√≥dulos de atenci√≥n basados en la arquitectura del modelo\n",
        "    target_modules = None\n",
        "\n",
        "    # Detectar tipo de modelo basado en atributos\n",
        "    if hasattr(modelo, \"gpt_neox\"):\n",
        "        # Para modelos tipo GPT-NeoX\n",
        "        target_modules = [\"attention.query_key_value\", \"attention.dense\"]\n",
        "    elif hasattr(modelo, \"transformer\"):\n",
        "        if hasattr(modelo.transformer, \"h\") and hasattr(modelo.transformer.h[0], \"attn\"):\n",
        "            # Para modelos GPT-2\n",
        "            target_modules = [\"attn.c_attn\", \"attn.c_proj\"]\n",
        "        elif hasattr(modelo.transformer, \"encoder\") and hasattr(modelo.transformer.encoder, \"layer\"):\n",
        "            # Para modelos tipo BERT/RoBERTa\n",
        "            target_modules = [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\", \"attention.output.dense\"]\n",
        "    elif hasattr(modelo, \"model\") and hasattr(modelo.model, \"decoder\"):\n",
        "        # Para modelos tipo T5\n",
        "        target_modules = [\"q\", \"v\"]\n",
        "\n",
        "    if not target_modules:\n",
        "        print(\"No se pudo detectar autom√°ticamente los m√≥dulos objetivo, usando configuraci√≥n gen√©rica\")\n",
        "        target_modules = [\"query\", \"value\", \"key\", \"out_proj\", \"dense\"]\n",
        "\n",
        "    print(f\"Configurando LoRA con target_modules: {target_modules}\")\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=target_modules,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM\n",
        "    )\n",
        "\n",
        "    modelo_peft = get_peft_model(modelo, lora_config)\n",
        "    modelo_peft.print_trainable_parameters()\n",
        "\n",
        "    return modelo_peft\n",
        "\n",
        "def guardar_modelo(modelo, tokenizador, ruta):\n",
        "    \"\"\"\n",
        "    Guarda el modelo y tokenizador en una ruta espec√≠fica.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a guardar\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        ruta (str): Ruta donde guardar\n",
        "    \"\"\"\n",
        "    modelo.save_pretrained(ruta)\n",
        "    tokenizador.save_pretrained(ruta)\n",
        "    print(f\"Modelo y tokenizador guardados en: {ruta}\")\n",
        "\n",
        "def cargar_modelo_personalizado(ruta):\n",
        "    \"\"\"\n",
        "    Carga un modelo personalizado desde una ruta espec√≠fica.\n",
        "\n",
        "    Args:\n",
        "        ruta (str): Ruta del modelo\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo, tokenizador, dispositivo)\n",
        "    \"\"\"\n",
        "    dispositivo = verificar_dispositivo()\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(ruta)\n",
        "    tokenizador = AutoTokenizer.from_pretrained(ruta)\n",
        "\n",
        "    # Configurar pad_token si no est√° definido\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        tokenizador.pad_token_id = tokenizador.eos_token_id\n",
        "\n",
        "    modelo.to(dispositivo)\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "# === Interfaz web con Gradio ===\n",
        "\n",
        "def crear_chatbot_con_memoria(modelo, tokenizador, dispositivo):\n",
        "    \"\"\"\n",
        "    Crea una instancia de chatbot con memoria de conversaci√≥n.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo de lenguaje\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        dispositivo: Dispositivo (CPU/GPU)\n",
        "\n",
        "    Returns:\n",
        "        Chatbot: Instancia del chatbot\n",
        "    \"\"\"\n",
        "    instrucciones = \"Eres un asistente virtual amable y servicial. Intenta dar respuestas √∫tiles y detalladas a las preguntas del usuario.\"\n",
        "    return Chatbot(modelo, tokenizador, dispositivo, instrucciones_sistema=instrucciones)\n",
        "\n",
        "def crear_interfaz_web(chatbot, parametros_predefinidos=None):\n",
        "    \"\"\"\n",
        "    Crea una interfaz web para el chatbot usando Gradio.\n",
        "\n",
        "    Args:\n",
        "        chatbot: Instancia del chatbot\n",
        "        parametros_predefinidos: Par√°metros de generaci√≥n predefinidos\n",
        "\n",
        "    Returns:\n",
        "        gr.Interface: Interfaz de Gradio\n",
        "    \"\"\"\n",
        "    # Estado para almacenar el historial de la conversaci√≥n\n",
        "    historial_chat = []\n",
        "\n",
        "    def responder_mensaje(mensaje, historial=None, temperatura=0.7, top_p=0.9, max_tokens=100):\n",
        "        if historial is None:\n",
        "            historial = []\n",
        "\n",
        "        # Configurar par√°metros de generaci√≥n personalizados\n",
        "        parametros = {\n",
        "            \"max_new_tokens\": max_tokens,\n",
        "            \"temperature\": temperatura,\n",
        "            \"top_p\": top_p,\n",
        "            \"do_sample\": True,\n",
        "            \"pad_token_id\": chatbot.tokenizador.pad_token_id,\n",
        "        }\n",
        "\n",
        "        # Obtener respuesta del chatbot\n",
        "        respuesta = chatbot.responder(mensaje, parametros_generacion=parametros)\n",
        "\n",
        "        # Actualizar historial\n",
        "        historial.append((mensaje, respuesta))\n",
        "        return \"\", historial\n",
        "\n",
        "    with gr.Blocks(title=\"Chatbot Personalizado con Memoria\") as interfaz:\n",
        "        gr.Markdown(\"# ü§ñ Chatbot Personalizado\")\n",
        "        gr.Markdown(\"Este chatbot utiliza un modelo de lenguaje personalizado con adaptadores LoRA y mantiene memoria de la conversaci√≥n.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=4):\n",
        "                chat_output = gr.Chatbot(height=400, label=\"Conversaci√≥n\")\n",
        "                mensaje_input = gr.Textbox(placeholder=\"Escribe tu mensaje aqu√≠...\", label=\"Mensaje\")\n",
        "                enviar_btn = gr.Button(\"Enviar\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Configuraci√≥n\")\n",
        "                temperatura_slider = gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.05, label=\"Temperatura\")\n",
        "                top_p_slider = gr.Slider(minimum=0.1, maximum=1.0, value=0.9, step=0.05, label=\"Top-p\")\n",
        "                max_tokens_slider = gr.Slider(minimum=10, maximum=200, value=100, step=10, label=\"M√°x. tokens\")\n",
        "                clear_btn = gr.Button(\"Nueva conversaci√≥n\")\n",
        "\n",
        "        # Eventos\n",
        "        enviar_btn.click(\n",
        "            fn=responder_mensaje,\n",
        "            inputs=[mensaje_input, chat_output, temperatura_slider, top_p_slider, max_tokens_slider],\n",
        "            outputs=[mensaje_input, chat_output]\n",
        "        )\n",
        "        mensaje_input.submit(\n",
        "            fn=responder_mensaje,\n",
        "            inputs=[mensaje_input, chat_output, temperatura_slider, top_p_slider, max_tokens_slider],\n",
        "            outputs=[mensaje_input, chat_output]\n",
        "        )\n",
        "        clear_btn.click(lambda: ([], []), outputs=[mensaje_input, chat_output])\n",
        "\n",
        "    return interfaz\n",
        "\n",
        "# === Despliegue ===\n",
        "\n",
        "def main_despliegue(modelo_base=\"Qwen/Qwen2.5-0.5B-Instruct\", usar_modelo_personalizado=False, ruta_modelo_personalizado=\"modelo_personalizado\"):\n",
        "    \"\"\"\n",
        "    Funci√≥n principal para el despliegue del chatbot.\n",
        "\n",
        "    Args:\n",
        "        modelo_base (str): Identificador del modelo base en HuggingFace\n",
        "        usar_modelo_personalizado (bool): Si True, carga un modelo personalizado guardado\n",
        "        ruta_modelo_personalizado (str): Ruta donde se encuentra el modelo personalizado\n",
        "    \"\"\"\n",
        "    if usar_modelo_personalizado:\n",
        "        print(f\"Cargando modelo personalizado desde: {ruta_modelo_personalizado}\")\n",
        "        modelo, tokenizador, dispositivo = cargar_modelo_personalizado(ruta_modelo_personalizado)\n",
        "    else:\n",
        "        print(f\"Cargando modelo base: {modelo_base}\")\n",
        "        modelo, tokenizador, dispositivo = cargar_modelo(modelo_base)\n",
        "\n",
        "        # Como opci√≥n: Aplicar PEFT/LoRA al modelo base\n",
        "        aplicar_peft = False\n",
        "        if aplicar_peft:\n",
        "            print(\"Aplicando adaptadores LoRA al modelo base...\")\n",
        "            modelo = configurar_peft(modelo)\n",
        "\n",
        "            # Guardar el modelo personalizado\n",
        "            guardar_modelo(modelo, tokenizador, ruta_modelo_personalizado)\n",
        "            print(f\"Modelo personalizado guardado en: {ruta_modelo_personalizado}\")\n",
        "\n",
        "    # Crear instancia del chatbot\n",
        "    chatbot = crear_chatbot_con_memoria(modelo, tokenizador, dispositivo)\n",
        "\n",
        "    # Crear y lanzar la interfaz web\n",
        "    print(\"Iniciando la interfaz web...\")\n",
        "    interfaz = crear_interfaz_web(chatbot)\n",
        "    interfaz.launch(share=True)  # share=True permite acceso p√∫blico a trav√©s de internet\n",
        "    print(\"Interfaz web iniciada.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Opciones de configuraci√≥n\n",
        "    options = {\n",
        "        \"modelo_base\": \"Qwen/Qwen2.5-0.5B-Instruct\",  # Modelo base a utilizar\n",
        "        \"usar_modelo_personalizado\": False,  # Cambiar a True para cargar un modelo personalizado\n",
        "        \"ruta_modelo_personalizado\": \"modelo_personalizado\"  # Ruta del modelo personalizado\n",
        "    }\n",
        "\n",
        "    main_despliegue(**options)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "AadMCR0kYTUz",
        "outputId": "784cfd65-70c2-4d41-dca7-767eae4a9e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando modelo base: Qwen/Qwen2.5-0.5B-Instruct\n",
            "Utilizando dispositivo: cpu\n",
            "Iniciando la interfaz web...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e2c36589aaad>:320: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chat_output = gr.Chatbot(height=400, label=\"Conversaci√≥n\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f53388deaee8008ece.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f53388deaee8008ece.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interfaz web iniciada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Preguntas Te√≥ricas***"
      ],
      "metadata": {
        "id": "wRJXBvfGVOB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*¬øCu√°les son las diferencias fundamentales entre los modelos encoder-only, decoder-only y encoder-decoder en el contexto de los chatbots conversacionales? Explique qu√© tipo de modelo ser√≠a m√°s adecuado para cada caso de uso y por qu√©.*"
      ],
      "metadata": {
        "id": "ABItK3PpVReZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Respuesta:**\n",
        "\n",
        "Basicamente, los modelos encoder-only, (como por ejemplo el modelo BERT), se enfocan en entender texto, por lo que son ideales para tareas tales como de clasificaci√≥n, detecci√≥n de intenci√≥n o an√°lisis de sentimientos, pero no generan respuestas. Por otro lado, los decoder-only, tales como GPT, generan texto de forma autoregresiva y son los m√°s adecuados para chatbots conversacionales abiertos, ya que pueden producir respuestas coherentes y lo m√°s importante, es que lo hacen turno tras turno. Finalmente, los modelos encoder-decoder, (as√≠ como T5 o BART), combinan la comprensi√≥n y generaci√≥n, siendo √∫tiles para tareas tan importantes como traducci√≥n, resumen o respuestas basadas en un contexto cerrado. Esto lo que nos permite concluir, es que para chatbots de conversaci√≥n abierta y continua, los decoder-only son una buena opci√≥n."
      ],
      "metadata": {
        "id": "WzNIFRm0Vdi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explique el concepto de \"temperatura\" en la generaci√≥n de texto con LLMs. ¬øC√≥mo afecta al comportamiento del chatbot y qu√© consideraciones debemos tener al ajustar este par√°metro para diferentes aplicaciones?*"
      ],
      "metadata": {
        "id": "xFEC7R-sVT3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Respuesta:**\n",
        "\n",
        "Como pudimos comprobar, la temperatura es un par√°metro que regula la aleatoriedad en la generaci√≥n de texto. Una temperatura baja (como por ejemplo una de 0.3) hace que el modelo sea m√°s preciso y repetitivo, ideal para tareas donde se requiere confiabilidad y exactitud. Por el contrario, una temperatura alta (como por ejemplo una de 1.0) promueve en el bot m√°s diversidad y creatividad, pero hay que tener en cuenta que esto puede llevar a errores o incoherencias. As√≠ que, el ajustar este valor depende del uso y del contexto de desarrollo: en asistentes t√©cnicos o educativos conviene usar temperaturas bajas, mientras que en aplicaciones creativas o de entretenimiento seria preferible el usar unas temperaturas m√°s altas.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7dXJLW6VsfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Describa las t√©cnicas principales para reducir el problema de \"alucinaciones\" en chatbots basados en LLMs. ¬øQu√© estrategias podemos implementar a nivel de inferencia y a nivel de prompt engineering para mejorar la precisi√≥n factual de las respuestas?*"
      ],
      "metadata": {
        "id": "e-n8gFd_VZw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Respuesta:**\n",
        "\n",
        "Para mitigar alucinaciones (o tambien conocidas como respuestas incorrectas), se pueden aplicar t√©cnicas tanto durante la inferencia como en el dise√±o del prompt. A nivel de inferencia, usar una temperatura baja, tales estrategias como el top-k/top-p sampling y modelos con recuperaci√≥n de informaci√≥n externa (como por ejemplo RAG) ayuda a mejorar la precisi√≥n. Debemos tambien tener en cuanta que en cuanto al prompt engineering, dar instrucciones expl√≠citas, incluir contexto relevante y utilizar ejemplos gu√≠a (que en ingles a esto se le conoce como few-shot prompting) nos puede ayudar a reducir errores. En ultima instancia, tambi√©n es √∫til pedirle al modelo que diga ‚Äúno s√©‚Äù si no tiene certeza, en lugar de inventar una respuesta o simplemente, como se le llama a una \"alucinaci√≥n\"."
      ],
      "metadata": {
        "id": "eq9pJ5ucV7j2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HECHO POR: Julian David Navarro G.\n",
        "\n",
        "IA - Mayo de 2025"
      ],
      "metadata": {
        "id": "PB7FnF5xWJtT"
      }
    }
  ]
}